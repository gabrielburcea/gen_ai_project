{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db942b07-f402-4c7e-a99e-be9372fa0ef8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pip install  -q -U llama-index PyPDF2\n",
    "#dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00d070bd-2072-4a20-99cb-45820da1cb94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pdf_path =\"/Volumes/workspace/default/test_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2375ad2b-a227-437d-be03-c1a396388d1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "pdf_path = \"/Volumes/workspace/default/test_data\"\n",
    "#Deine the output table name\n",
    "table_name = \"default.test_data_pdf_index\"\n",
    "\n",
    "# Read all ODF files in the folder\n",
    "df = (\n",
    "    spark.read.format(\"binaryFile\")\n",
    "    .option(\"recursiveFileLookup\", \"true\")\n",
    "    .load(pdf_path)\n",
    ")\n",
    "\n",
    "# Write the Dataframe as a table (for file indexing/logging)\n",
    "df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf34202e-5f91-47a0-97d7-e5b7e3e369fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.limit(20).select(\"path\", \"length\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "219176c6-3307-4357-985a-86387375fa2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parse_bytes_pypdf(pdf_bytes):\n",
    "    from PyPDF2 import PdfReader\n",
    "    import io\n",
    "    try:\n",
    "        reader = PdfReader(io.BytesIO(pdf_bytes))\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "try:   \n",
    "    with open(f\"{pdf_path}/\", mode=\"rb\") as pdf_file:\n",
    "        pdf_bytes = pdf.read()\n",
    "        print(\"PDF read succesfully\")\n",
    "        doc = parse_bytes_pypdf(pdf.read())\n",
    "        if doc: \n",
    "            print(doc)\n",
    "        print(\"File not found\")\n",
    "except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6e686beb-cefd-41b6-a70c-ea05eb7b3c42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#pip install pytesseract pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13ec06f3-a481-496f-8025-99e1de45a583",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parse_bytes_pypdf(pdf_bytes):\n",
    "    from PyPDF2 import PdfReader\n",
    "    import io\n",
    "    try:\n",
    "        reader = PdfReader(io.BytesIO(pdf_bytes))\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \"\\n\"\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing PDF: {e}\")\n",
    "        return None\n",
    "\n",
    "pdf_path = \"/Volumes/workspace/default/test_data\"\n",
    "\n",
    "try:   \n",
    "    with open(f\"{pdf_path}/Fast_Resampling_three_dimensional_points_clouds.pdf\", mode=\"rb\") as pdf_file:\n",
    "        pdf_bytes = pdf_file.read()\n",
    "        print(\"PDF read successfully\")\n",
    "        doc = parse_bytes_pypdf(pdf_bytes)\n",
    "        if doc: \n",
    "            print(doc)\n",
    "        else:\n",
    "            print(\"No text extracted from PDF\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d860af3e-4a6d-4bd4-810f-06be2431a0a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8814c06-03ca-49e1-808c-136ef1a2829e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.schema import Document\n",
    "from llama_index.core.utils import set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Iterator\n",
    "from pyspark.sql.functions import col, udf, length, pandas_udf, explode\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d9697e88-9186-44c3-98c8-ba7c155c45d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.deployments import get_deploy_client\n",
    "\n",
    "\n",
    "# gte-large-en Foundation models are available using the /serving-endpoints/databricks-gte-large-en/invocations api. \n",
    "deploy_client = get_deploy_client(\"databricks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c512f54-c65e-49ed-bf15-62acc0951582",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define a function to splot the text content into chuncks\n",
    "@pandas_udf(\"array<string>\")\n",
    "def read_as_chunck(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "    #Set llamma2 as tokenizer\n",
    "    set_global_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(\"hf-internal-testing/llama-tokenizer\")\n",
    "    )\n",
    "    # Sentence splitter from llama_index to split on sentences\n",
    "    splitter = SentenceSplitter(chunk_size=500, chunck_overlap = 50)\n",
    "\n",
    "    def extract_and_split(b):\n",
    "        txt = parse_bytes_pypdf(b) \n",
    "        if txt is None: \n",
    "            return[]\n",
    "        nodes = splitter.get_nodes_from_documentss([Document(text=txt)])\n",
    "        return[n.text for n in nodes]\n",
    "    for x in batch_iter:\n",
    "        yield x.apply(extract_and_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fa24b4d-ce66-48fa-9bfd-1c9bb1756098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Set the Huggine Face cache directory to a writable location\n",
    "\n",
    "df_chunks = (df\n",
    "             .withColumn(\"content\", explode(array(read_as_chunk(\"content\"))))\n",
    "             .selectExpr(\"path as pdf_name\", \"content\")\n",
    "             )\n",
    "display(df_chunks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24a83e6e-d277-41be-bdf3-d3a822324a16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS workspace.default.pdf_text_embeddings (\n",
    "  id BIGINT GENERATED BY DEFAULT AS IDENTITY,\n",
    "  pdf_name STRING,\n",
    "  content STRING,\n",
    "  embedding ARRAY <FLOAT>\n",
    "  -- NOTE: the table has to be CDC because VectorSearch is using DLT that is requiring CDC state\n",
    "  ) TBLPROPERTIES (delta.enableChangeDataFeed = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deea348c-e370-48ec-83d7-2b1733af2e91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@pandas_udf(\"array<float>\")\n",
    "def get_embedding(contents: pd.Series) -> pd.Series:\n",
    "    import mlflow.deployments\n",
    "    deploy_client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "    def get_embeddings(batch):\n",
    "        # NOTE: this will fail if an exception is thrown during embedding creation (add try/except if needed) \n",
    "        response = deploy_client.predict(endpoint=\"databricks-gte-large-en\", inputs={\"input\": batch})\n",
    "        return [e[\"embedding\"] for e in response.data]\n",
    "\n",
    "    # splitting the contents into batches of 150 items each, since the embedding model takes at most 150 inputs per request.\n",
    "    max_batch_size = 150\n",
    "    batches = [contents.iloc[i:i + max_batch_size] for i in range(0, len(contents), max_batch_size)]\n",
    "\n",
    "    # process each batch and collect the results\n",
    "    all_embeddings = []\n",
    "    for batch in batches:\n",
    "        all_embeddings += get_embeddings(batch.tolist())\n",
    "\n",
    "    return pd.Series(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1db2733-91f9-4d22-aef2-4f66e7217fd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df_chunk_emd = (df_chunks\n",
    "                .withColumn(\"embedding\", get_embedding(\"content\"))\n",
    "                .selectExpr(\"pdf_name\", \"content\", \"embedding\")\n",
    "                )\n",
    "#display(df_chunk_emd)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7498953739293720,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "reading_cleaning_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
